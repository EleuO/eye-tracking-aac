import { ref, reactive, onMounted, onUnmounted } from 'vue'

/**
 * ã‚·ãƒ³ãƒ—ãƒ«é¡”æ¤œå‡ºãƒ™ãƒ¼ã‚¹è¦–ç·šè¿½è·¡ã‚·ã‚¹ãƒ†ãƒ 
 * MediaPipeã®è¤‡é›‘ãªåˆæœŸåŒ–ã‚’å›é¿ã—ã€ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–APIã‚’æ´»ç”¨
 */
export function useSimpleFaceTracker() {
  // çŠ¶æ…‹ç®¡ç†
  const isInitialized = ref(false)
  const isTracking = ref(false)
  const faceDetected = ref(false)
  const error = ref(null)
  
  // é¡”æ¤œå‡ºãƒ‡ãƒ¼ã‚¿
  const faceData = reactive({
    x: 0,           // é¡”ä¸­å¿ƒXåº§æ¨™
    y: 0,           // é¡”ä¸­å¿ƒYåº§æ¨™
    width: 0,       // é¡”ã®å¹…
    height: 0,      // é¡”ã®é«˜ã•
    confidence: 0,  // æ¤œå‡ºä¿¡é ¼åº¦
    headPose: {     // é ­éƒ¨å§¿å‹¢æ¨å®š
      yaw: 0,       // å·¦å³å›è»¢ (-45Â° to +45Â°)
      pitch: 0,     // ä¸Šä¸‹å›è»¢ (-30Â° to +30Â°)
      roll: 0       // å‚¾ã
    }
  })
  
  // ã‚«ãƒ¡ãƒ©è¦ç´ 
  const videoElement = ref(null)
  const canvasElement = ref(null)
  const canvasCtx = ref(null)
  
  // è¨­å®š
  const settings = reactive({
    targetFPS: 30,
    smoothingFactor: 0.3, // ã‚ˆã‚Šæ•æ„Ÿã«ï¼ˆ0.7 â†’ 0.3ï¼‰
    confidenceThreshold: 0.3, // ã‚ˆã‚Šä½ã„é–¾å€¤ã§æ¤œå‡ºï¼ˆ0.5 â†’ 0.3ï¼‰
    debugMode: true
  })
  
  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
  const stats = reactive({
    fps: 0,
    frameCount: 0,
    lastUpdate: Date.now()
  })
  
  // Face Detection API
  let faceDetector = null
  let animationFrame = null
  let lastFrameTime = 0
  
  /**
   * ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–Face DetectionåˆæœŸåŒ–
   */
  const initializeFaceDetection = async () => {
    try {
      console.log('ğŸš€ ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–é¡”æ¤œå‡ºåˆæœŸåŒ–é–‹å§‹')
      
      // ãƒ–ãƒ©ã‚¦ã‚¶å¯¾å¿œãƒã‚§ãƒƒã‚¯
      if (!('FaceDetector' in window)) {
        console.log('âš ï¸ ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–FaceDetectorãŒåˆ©ç”¨ã§ãã¾ã›ã‚“')
        await initializeFallbackDetection()
        return
      }
      
      // Face DetectoråˆæœŸåŒ–
      faceDetector = new FaceDetector({
        maxDetectedFaces: 1,
        fastMode: true
      })
      
      console.log('âœ… ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–é¡”æ¤œå‡ºåˆæœŸåŒ–å®Œäº†')
      isInitialized.value = true
      
    } catch (err) {
      console.error('âŒ ãƒã‚¤ãƒ†ã‚£ãƒ–é¡”æ¤œå‡ºåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', err)
      await initializeFallbackDetection()
    }
  }
  
  /**
   * ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: OpenCVãƒ™ãƒ¼ã‚¹é¡”æ¤œå‡º
   */
  const initializeFallbackDetection = async () => {
    console.log('ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é¡”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...')
    
    try {
      // ã‚·ãƒ³ãƒ—ãƒ«ãªé¡”é ˜åŸŸæ¨å®šã‚·ã‚¹ãƒ†ãƒ 
      await initializeSimpleFaceDetection()
      
      console.log('âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é¡”æ¤œå‡ºåˆæœŸåŒ–å®Œäº†')
      isInitialized.value = true
      
    } catch (err) {
      console.error('âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆæœŸåŒ–å¤±æ•—:', err)
      error.value = 'é¡”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ'
    }
  }
  
  /**
   * è¶…ã‚·ãƒ³ãƒ—ãƒ«é¡”æ¤œå‡ºï¼ˆè‰²ãƒ™ãƒ¼ã‚¹ï¼‰
   */
  const initializeSimpleFaceDetection = async () => {
    console.log('ğŸ¯ ã‚·ãƒ³ãƒ—ãƒ«é¡”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–')
    
    // è‚Œè‰²ãƒ™ãƒ¼ã‚¹é¡”é ˜åŸŸæ¨å®šã‚·ã‚¹ãƒ†ãƒ 
    // HSVè‰²ç©ºé–“ã§è‚Œè‰²ç¯„å›²ã‚’æ¤œå‡º
    isInitialized.value = true
  }
  
  /**
   * ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹
   */
  const startCamera = async (constraints = {}) => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640, max: 1280 },
          height: { ideal: 480, max: 720 },
          frameRate: { ideal: 30 },
          facingMode: 'user',
          ...constraints
        }
      })
      
      if (videoElement.value) {
        videoElement.value.srcObject = stream
        await videoElement.value.play()
        
        console.log('ğŸ“¹ ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹:', {
          width: videoElement.value.videoWidth,
          height: videoElement.value.videoHeight
        })
      }
      
      return stream
    } catch (err) {
      console.error('âŒ ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼:', err)
      error.value = `ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: ${err.message}`
      throw err
    }
  }
  
  /**
   * è¦–ç·šè¿½è·¡é–‹å§‹
   */
  const startTracking = async (videoEl, canvasEl) => {
    try {
      videoElement.value = videoEl
      canvasElement.value = canvasEl
      
      if (canvasEl) {
        canvasCtx.value = canvasEl.getContext('2d')
      }
      
      if (!isInitialized.value) {
        await initializeFaceDetection()
      }
      
      await startCamera()
      
      // ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†é–‹å§‹
      startFrameProcessing()
      
      isTracking.value = true
      console.log('âœ… ã‚·ãƒ³ãƒ—ãƒ«è¦–ç·šè¿½è·¡é–‹å§‹')
      
    } catch (err) {
      console.error('âŒ è¿½è·¡é–‹å§‹ã‚¨ãƒ©ãƒ¼:', err)
      error.value = `è¿½è·¡é–‹å§‹å¤±æ•—: ${err.message}`
    }
  }
  
  /**
   * ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ãƒ«ãƒ¼ãƒ—
   */
  const startFrameProcessing = () => {
    const processFrame = async (timestamp) => {
      if (!isTracking.value) return
      
      // FPSåˆ¶å¾¡
      if (timestamp - lastFrameTime < 1000 / settings.targetFPS) {
        animationFrame = requestAnimationFrame(processFrame)
        return
      }
      
      lastFrameTime = timestamp
      
      try {
        await detectFace()
        updateStats()
      } catch (err) {
        console.error('ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼:', err)
      }
      
      animationFrame = requestAnimationFrame(processFrame)
    }
    
    animationFrame = requestAnimationFrame(processFrame)
  }
  
  /**
   * é¡”æ¤œå‡ºå®Ÿè¡Œ
   */
  const detectFace = async () => {
    if (!videoElement.value || !canvasElement.value) return
    
    const video = videoElement.value
    const canvas = canvasElement.value
    const ctx = canvasCtx.value
    
    // ãƒ“ãƒ‡ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒ³ãƒã‚¹ã«æç”»
    canvas.width = video.videoWidth
    canvas.height = video.videoHeight
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height)
    
    if (faceDetector) {
      // ãƒ–ãƒ©ã‚¦ã‚¶ãƒã‚¤ãƒ†ã‚£ãƒ–é¡”æ¤œå‡º
      try {
        const faces = await faceDetector.detect(canvas)
        processFaceDetectionResults(faces)
      } catch (err) {
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œå‡º
        processSimpleFaceDetection(ctx, canvas)
      }
    } else {
      // ã‚·ãƒ³ãƒ—ãƒ«é¡”æ¤œå‡º
      processSimpleFaceDetection(ctx, canvas)
    }
  }
  
  /**
   * é¡”æ¤œå‡ºçµæœå‡¦ç†
   */
  const processFaceDetectionResults = (faces) => {
    if (faces && faces.length > 0) {
      const face = faces[0]
      
      faceDetected.value = true
      
      // é¡”ã®ä½ç½®ãƒ»ã‚µã‚¤ã‚ºæ›´æ–°
      const bbox = face.boundingBox
      faceData.x = bbox.x + bbox.width / 2
      faceData.y = bbox.y + bbox.height / 2
      faceData.width = bbox.width
      faceData.height = bbox.height
      faceData.confidence = face.confidence || 0.8
      
      // ç°¡æ˜“é ­éƒ¨å§¿å‹¢æ¨å®š
      calculateSimpleHeadPose(bbox)
      
      // ãƒ‡ãƒãƒƒã‚°æç”»
      if (settings.debugMode) {
        drawFaceDebug(bbox)
      }
      
    } else {
      faceDetected.value = false
      resetFaceData()
    }
  }
  
  /**
   * ã‚·ãƒ³ãƒ—ãƒ«é¡”æ¤œå‡ºï¼ˆè‰²ãƒ™ãƒ¼ã‚¹ï¼‰
   */
  const processSimpleFaceDetection = (ctx, canvas) => {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height)
    const data = imageData.data
    
    let skinPixelCount = 0
    let totalPixels = 0
    let centroidX = 0
    let centroidY = 0
    
    // è‚Œè‰²æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
    for (let i = 0; i < data.length; i += 4) {
      const r = data[i]
      const g = data[i + 1]
      const b = data[i + 2]
      
      // è‚Œè‰²åˆ¤å®šï¼ˆç°¡æ˜“HSVç¯„å›²ï¼‰
      if (isSkinColor(r, g, b)) {
        const pixelIndex = i / 4
        const x = pixelIndex % canvas.width
        const y = Math.floor(pixelIndex / canvas.width)
        
        centroidX += x
        centroidY += y
        skinPixelCount++
      }
      totalPixels++
    }
    
    // é¡”ä¸­å¿ƒæ¨å®š
    if (skinPixelCount > totalPixels * 0.05) { // 5%ä»¥ä¸ŠãŒè‚Œè‰²
      faceDetected.value = true
      
      faceData.x = centroidX / skinPixelCount
      faceData.y = centroidY / skinPixelCount
      faceData.width = Math.sqrt(skinPixelCount) * 2
      faceData.height = Math.sqrt(skinPixelCount) * 2.5
      faceData.confidence = Math.min(skinPixelCount / (totalPixels * 0.1), 1)
      
      // ç°¡æ˜“é ­éƒ¨å§¿å‹¢æ¨å®š
      calculateSimpleHeadPose({
        x: faceData.x - faceData.width / 2,
        y: faceData.y - faceData.height / 2,
        width: faceData.width,
        height: faceData.height
      })
      
    } else {
      faceDetected.value = false
      resetFaceData()
    }
  }
  
  /**
   * æ”¹è‰¯ç‰ˆè‚Œè‰²åˆ¤å®šï¼ˆã‚ˆã‚Šåºƒç¯„å›²ã®è‚Œè‰²ã«å¯¾å¿œï¼‰
   */
  const isSkinColor = (r, g, b) => {
    // ã‚ˆã‚Šåºƒç¯„å›²ã®è‚Œè‰²åˆ¤å®š
    const rgbCondition1 = (
      r > 95 && g > 40 && b > 20 &&
      r > g && r > b &&
      Math.abs(r - g) > 15 &&
      r - b > 15
    )
    
    // è¿½åŠ ã®è‚Œè‰²åˆ¤å®šï¼ˆã‚¢ã‚¸ã‚¢ç³»ã€ç™½äººç³»ï¼‰
    const rgbCondition2 = (
      r > 80 && g > 50 && b > 30 &&
      r > b && g > b &&
      Math.abs(r - g) < 30
    )
    
    // HSVå¤‰æ›ã«ã‚ˆã‚‹åˆ¤å®š
    const max = Math.max(r, g, b)
    const min = Math.min(r, g, b)
    const delta = max - min
    
    let h = 0
    if (delta > 0) {
      if (max === r) h = ((g - b) / delta) % 6
      else if (max === g) h = (b - r) / delta + 2
      else h = (r - g) / delta + 4
      h *= 60
    }
    
    const s = max === 0 ? 0 : delta / max
    const v = max / 255
    
    // è‚Œè‰²ã®HSVç¯„å›²
    const hsvCondition = (
      ((h >= 0 && h <= 30) || (h >= 330 && h <= 360)) &&
      s >= 0.2 && s <= 0.7 &&
      v >= 0.4 && v <= 0.95
    )
    
    return rgbCondition1 || rgbCondition2 || hsvCondition
  }
  
  /**
   * æ”¹è‰¯ç‰ˆé ­éƒ¨å§¿å‹¢æ¨å®šï¼ˆã‚ˆã‚Šé«˜ç²¾åº¦ï¼‰
   */
  const calculateSimpleHeadPose = (bbox) => {
    const canvas = canvasElement.value
    if (!canvas) return
    
    const faceCenterX = bbox.x + bbox.width / 2
    const faceCenterY = bbox.y + bbox.height / 2
    
    const screenCenterX = canvas.width / 2
    const screenCenterY = canvas.height / 2
    
    // æ­£è¦åŒ–åº§æ¨™ (-1 to 1) - ã‚ˆã‚Šæ„Ÿåº¦ã‚’é«˜ã‚ã‚‹
    const normalizedX = (faceCenterX - screenCenterX) / (screenCenterX * 0.6) // æ„Ÿåº¦UP
    const normalizedY = (faceCenterY - screenCenterY) / (screenCenterY * 0.6) // æ„Ÿåº¦UP
    
    // ç¯„å›²åˆ¶é™
    const clampedX = Math.max(-1, Math.min(1, normalizedX))
    const clampedY = Math.max(-1, Math.min(1, normalizedY))
    
    // é ­éƒ¨å§¿å‹¢ã«å¤‰æ›ï¼ˆåº¦å˜ä½ï¼‰ - ã‚ˆã‚Šå¤§ããªè§’åº¦ç¯„å›²
    faceData.headPose.yaw = clampedX * 45  // -45Â° to +45Â°
    faceData.headPose.pitch = clampedY * 35 // -35Â° to +35Â°
    faceData.headPose.roll = 0 // ç°¡ç•¥åŒ–
    
    // ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
    applySmoothing()
    
    // ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
    if (settings.debugMode && Date.now() % 1000 < 50) {
      console.log(`é ­éƒ¨å§¿å‹¢: Yaw=${Math.round(faceData.headPose.yaw)}Â°, Pitch=${Math.round(faceData.headPose.pitch)}Â°`)
    }
  }
  
  /**
   * ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
   */
  let previousPose = { yaw: 0, pitch: 0, roll: 0 }
  
  const applySmoothing = () => {
    const factor = settings.smoothingFactor
    
    faceData.headPose.yaw = lerp(previousPose.yaw, faceData.headPose.yaw, factor)
    faceData.headPose.pitch = lerp(previousPose.pitch, faceData.headPose.pitch, factor)
    faceData.headPose.roll = lerp(previousPose.roll, faceData.headPose.roll, factor)
    
    previousPose = { ...faceData.headPose }
  }
  
  /**
   * ç·šå½¢è£œé–“
   */
  const lerp = (a, b, t) => a + (b - a) * t
  
  /**
   * ãƒ‡ãƒãƒƒã‚°æç”»
   */
  const drawFaceDebug = (bbox) => {
    const ctx = canvasCtx.value
    if (!ctx) return
    
    // é¡”ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
    ctx.strokeStyle = '#00ff00'
    ctx.lineWidth = 2
    ctx.strokeRect(bbox.x, bbox.y, bbox.width, bbox.height)
    
    // ä¸­å¿ƒç‚¹
    ctx.fillStyle = '#ff0000'
    ctx.beginPath()
    ctx.arc(faceData.x, faceData.y, 5, 0, 2 * Math.PI)
    ctx.fill()
    
    // ä¿¡é ¼åº¦è¡¨ç¤º
    ctx.fillStyle = '#00ff00'
    ctx.font = '16px Arial'
    ctx.fillText(`ä¿¡é ¼åº¦: ${Math.round(faceData.confidence * 100)}%`, 10, 30)
    ctx.fillText(`FPS: ${stats.fps}`, 10, 50)
    
    // é ­éƒ¨å§¿å‹¢è¡¨ç¤º
    ctx.fillText(`Yaw: ${Math.round(faceData.headPose.yaw)}Â°`, 10, 70)
    ctx.fillText(`Pitch: ${Math.round(faceData.headPose.pitch)}Â°`, 10, 90)
  }
  
  /**
   * ãƒ•ã‚§ã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ
   */
  const resetFaceData = () => {
    faceData.x = 0
    faceData.y = 0
    faceData.width = 0
    faceData.height = 0
    faceData.confidence = 0
    faceData.headPose = { yaw: 0, pitch: 0, roll: 0 }
  }
  
  /**
   * çµ±è¨ˆæ›´æ–°
   */
  const updateStats = () => {
    stats.frameCount++
    const now = Date.now()
    
    if (now - stats.lastUpdate >= 1000) {
      stats.fps = Math.round(stats.frameCount * 1000 / (now - stats.lastUpdate))
      stats.frameCount = 0
      stats.lastUpdate = now
    }
  }
  
  /**
   * è¿½è·¡åœæ­¢
   */
  const stopTracking = () => {
    isTracking.value = false
    
    if (animationFrame) {
      cancelAnimationFrame(animationFrame)
      animationFrame = null
    }
    
    if (videoElement.value && videoElement.value.srcObject) {
      const tracks = videoElement.value.srcObject.getTracks()
      tracks.forEach(track => track.stop())
      videoElement.value.srcObject = null
    }
    
    resetFaceData()
    console.log('â¹ï¸ ã‚·ãƒ³ãƒ—ãƒ«è¦–ç·šè¿½è·¡åœæ­¢')
  }
  
  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  onUnmounted(() => {
    stopTracking()
  })
  
  return {
    // çŠ¶æ…‹
    isInitialized,
    isTracking,
    faceDetected,
    error,
    faceData,
    settings,
    stats,
    
    // ãƒ¡ã‚½ãƒƒãƒ‰
    initializeFaceDetection,
    startTracking,
    stopTracking,
    startCamera
  }
}