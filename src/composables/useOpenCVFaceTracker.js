import { ref, reactive, onMounted, onUnmounted } from 'vue'
import { useAdvancedFaceAnalyzer } from './useAdvancedFaceAnalyzer.js'

/**
 * OpenCV.jsãƒ™ãƒ¼ã‚¹ç¢ºå®Ÿãªé¡”æ¤œå‡ºãƒ»è¦–ç·šè¿½è·¡ã‚·ã‚¹ãƒ†ãƒ 
 * ã‚ˆã‚Šæ­£ç¢ºã§å®‰å®šã—ãŸé¡”æ¤œå‡ºã¨ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯è§£æ
 * ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒè§£æçµ±åˆç‰ˆ
 */
export function useOpenCVFaceTracker() {
  // çŠ¶æ…‹ç®¡ç†
  const isInitialized = ref(false)
  const isTracking = ref(false)
  const faceDetected = ref(false)
  const error = ref(null)
  
  // é¡”æ¤œå‡ºãƒ‡ãƒ¼ã‚¿
  const faceData = reactive({
    x: 0,           // é¡”ä¸­å¿ƒXåº§æ¨™
    y: 0,           // é¡”ä¸­å¿ƒYåº§æ¨™
    width: 0,       // é¡”ã®å¹…
    height: 0,      // é¡”ã®é«˜ã•
    confidence: 0,  // æ¤œå‡ºä¿¡é ¼åº¦
    landmarks: {},  // é¡”ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯
    headPose: {     // é ­éƒ¨å§¿å‹¢
      yaw: 0,       // å·¦å³å›è»¢ (-45Â° to +45Â°)
      pitch: 0,     // ä¸Šä¸‹å›è»¢ (-30Â° to +30Â°)
      roll: 0       // å‚¾ã
    }
  })
  
  // ã‚«ãƒ¡ãƒ©è¦ç´ 
  const videoElement = ref(null)
  const canvasElement = ref(null)
  const canvasCtx = ref(null)
  
  // è¨­å®š
  const settings = reactive({
    targetFPS: 30,
    smoothingFactor: 0.2, // ã‚ˆã‚Šæ•æ„Ÿã«
    confidenceThreshold: 0.3,
    debugMode: true
  })
  
  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
  const stats = reactive({
    fps: 0,
    frameCount: 0,
    lastUpdate: Date.now(),
    detectionMethod: 'none'
  })
  
  // OpenCVå¤‰æ•°
  let cv = null
  let classifier = null
  let animationFrame = null
  let lastFrameTime = 0
  
  // ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒè§£æã‚·ã‚¹ãƒ†ãƒ 
  const faceAnalyzer = useAdvancedFaceAnalyzer()
  
  /**
   * OpenCV.jsåˆæœŸåŒ–
   */
  const initializeOpenCV = async () => {
    try {
      console.log('ğŸš€ OpenCV.jsåˆæœŸåŒ–é–‹å§‹')
      
      // OpenCV.jsã‚’CDNã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
      if (typeof window.cv === 'undefined') {
        await loadOpenCVFromCDN()
      }
      
      cv = window.cv
      
      // é¡”æ¤œå‡ºåˆ†é¡å™¨ã®åˆæœŸåŒ–
      await initializeFaceClassifier()
      
      console.log('âœ… OpenCV.jsåˆæœŸåŒ–å®Œäº†')
      stats.detectionMethod = 'OpenCV'
      isInitialized.value = true
      
    } catch (err) {
      console.error('âŒ OpenCVåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', err)
      // OpenCVãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      await initializeBasicDetection()
    }
  }
  
  /**
   * CDNã‹ã‚‰OpenCV.jsã‚’ãƒ­ãƒ¼ãƒ‰
   */
  const loadOpenCVFromCDN = async () => {
    return new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = 'https://docs.opencv.org/4.8.0/opencv.js'
      script.async = true
      
      script.onload = () => {
        // OpenCVã®åˆæœŸåŒ–å®Œäº†ã‚’å¾…ã¤
        const waitForOpenCV = () => {
          if (typeof window.cv !== 'undefined' && window.cv.Mat) {
            console.log('âœ… OpenCV.jsèª­ã¿è¾¼ã¿å®Œäº†')
            resolve()
          } else {
            setTimeout(waitForOpenCV, 100)
          }
        }
        waitForOpenCV()
      }
      
      script.onerror = () => reject(new Error('OpenCV.jsèª­ã¿è¾¼ã¿å¤±æ•—'))
      document.head.appendChild(script)
    })
  }
  
  /**
   * é¡”æ¤œå‡ºåˆ†é¡å™¨ã®åˆæœŸåŒ–
   */
  const initializeFaceClassifier = async () => {
    try {
      // Haar CASCADEåˆ†é¡å™¨ã‚’ãƒ­ãƒ¼ãƒ‰
      const classifierUrl = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'
      
      const response = await fetch(classifierUrl)
      const xmlData = await response.text()
      
      // OpenCVãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
      cv.FS_createDataFile('/', 'haarcascade_frontalface_default.xml', xmlData, true, false, false)
      
      // åˆ†é¡å™¨ã‚’åˆæœŸåŒ–
      classifier = new cv.CascadeClassifier()
      classifier.load('haarcascade_frontalface_default.xml')
      
      console.log('âœ… é¡”æ¤œå‡ºåˆ†é¡å™¨åˆæœŸåŒ–å®Œäº†')
      
    } catch (err) {
      console.error('âŒ åˆ†é¡å™¨åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', err)
      throw err
    }
  }
  
  /**
   * ãƒ™ãƒ¼ã‚·ãƒƒã‚¯é¡”æ¤œå‡ºï¼ˆOpenCVå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
   */
  const initializeBasicDetection = async () => {
    console.log('ğŸ”„ ãƒ™ãƒ¼ã‚·ãƒƒã‚¯é¡”æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ')
    stats.detectionMethod = 'Basic'
    isInitialized.value = true
  }
  
  /**
   * ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹ï¼ˆUSBã‚«ãƒ¡ãƒ©å¯¾å¿œï¼‰
   */
  const startCamera = async (constraints = {}) => {
    try {
      console.log('ğŸ“¹ ã‚«ãƒ¡ãƒ©åˆ¶ç´„:', constraints)
      
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640, max: 1280 },
          height: { ideal: 480, max: 720 },
          frameRate: { ideal: 30 },
          facingMode: constraints.deviceId ? undefined : 'user', // deviceIdæŒ‡å®šæ™‚ã¯facingModeç„¡åŠ¹
          ...constraints
        }
      })
      
      if (videoElement.value) {
        videoElement.value.srcObject = stream
        await videoElement.value.play()
        
        console.log('ğŸ“¹ ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹:', {
          width: videoElement.value.videoWidth,
          height: videoElement.value.videoHeight
        })
      }
      
      return stream
    } catch (err) {
      console.error('âŒ ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼:', err)
      error.value = `ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: ${err.message}`
      throw err
    }
  }
  
  /**
   * è¦–ç·šè¿½è·¡é–‹å§‹ï¼ˆUSBã‚«ãƒ¡ãƒ©å¯¾å¿œç‰ˆï¼‰
   */
  const startTracking = async (videoEl, canvasEl, cameraConstraints = {}) => {
    try {
      videoElement.value = videoEl
      canvasElement.value = canvasEl
      
      if (canvasEl) {
        canvasCtx.value = canvasEl.getContext('2d')
      }
      
      if (!isInitialized.value) {
        await initializeOpenCV()
      }
      
      await startCamera(cameraConstraints)
      
      // ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†é–‹å§‹
      startFrameProcessing()
      
      isTracking.value = true
      console.log('âœ… OpenCVè¦–ç·šè¿½è·¡é–‹å§‹')
      
    } catch (err) {
      console.error('âŒ è¿½è·¡é–‹å§‹ã‚¨ãƒ©ãƒ¼:', err)
      error.value = `è¿½è·¡é–‹å§‹å¤±æ•—: ${err.message}`
    }
  }
  
  /**
   * ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ãƒ«ãƒ¼ãƒ—
   */
  const startFrameProcessing = () => {
    const processFrame = async (timestamp) => {
      if (!isTracking.value) return
      
      // FPSåˆ¶å¾¡
      if (timestamp - lastFrameTime < 1000 / settings.targetFPS) {
        animationFrame = requestAnimationFrame(processFrame)
        return
      }
      
      lastFrameTime = timestamp
      
      try {
        if (cv && classifier) {
          await detectFaceWithOpenCV()
        } else {
          await detectFaceBasic()
        }
        updateStats()
      } catch (err) {
        console.error('ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼:', err)
      }
      
      animationFrame = requestAnimationFrame(processFrame)
    }
    
    animationFrame = requestAnimationFrame(processFrame)
  }
  
  /**
   * OpenCVã«ã‚ˆã‚‹é¡”æ¤œå‡º
   */
  const detectFaceWithOpenCV = async () => {
    if (!videoElement.value || !canvasElement.value || !cv || !classifier) return
    
    const video = videoElement.value
    const canvas = canvasElement.value
    const ctx = canvasCtx.value
    
    // ãƒ“ãƒ‡ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒ³ãƒã‚¹ã«æç”»
    canvas.width = video.videoWidth
    canvas.height = video.videoHeight
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height)
    
    try {
      // OpenCVãƒãƒƒãƒˆã‚’ä½œæˆ
      const src = cv.imread(canvas)
      const gray = new cv.Mat()
      const faces = new cv.RectVector()
      
      // ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
      cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY, 0)
      
      // é¡”æ¤œå‡ºå®Ÿè¡Œ
      classifier.detectMultiScale(gray, faces, 1.1, 3, 0, new cv.Size(30, 30), new cv.Size(0, 0))
      
      if (faces.size() > 0) {
        // æœ€å¤§ã®é¡”ã‚’é¸æŠ
        let maxArea = 0
        let bestFace = null
        
        for (let i = 0; i < faces.size(); i++) {
          const face = faces.get(i)
          const area = face.width * face.height
          if (area > maxArea) {
            maxArea = area
            bestFace = face
          }
        }
        
        if (bestFace) {
          // ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒè§£æã§ã‚ˆã‚Šæ­£ç¢ºãªç›®ã®ä½ç½®æ¤œå‡º
          await processFaceDetectionWithAdvancedAnalysis(bestFace, src)
          
          // ğŸ¯ é«˜åº¦ãªè§£æçµæœã®æç”»
          drawAdvancedFaceAnalysis(ctx, bestFace)
        }
      } else {
        faceDetected.value = false
        resetFaceData()
      }
      
      // ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
      src.delete()
      gray.delete()
      faces.delete()
      
    } catch (err) {
      console.error('OpenCVé¡”æ¤œå‡ºã‚¨ãƒ©ãƒ¼:', err)
      // ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ™ãƒ¼ã‚·ãƒƒã‚¯æ¤œå‡ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      await detectFaceBasic()
    }
  }
  
  /**
   * ãƒ™ãƒ¼ã‚·ãƒƒã‚¯é¡”æ¤œå‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
   */
  const detectFaceBasic = async () => {
    // ã‚ˆã‚Šç°¡å˜ãªé¡”æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    if (!videoElement.value || !canvasElement.value) return
    
    const video = videoElement.value
    const canvas = canvasElement.value
    const ctx = canvasCtx.value
    
    canvas.width = video.videoWidth
    canvas.height = video.videoHeight
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height)
    
    // ç°¡æ˜“çš„ãªé¡”é ˜åŸŸæ¨å®šï¼ˆç”»é¢ä¸­å¤®ä»˜è¿‘ï¼‰
    const faceWidth = canvas.width * 0.3
    const faceHeight = canvas.height * 0.4
    const faceX = (canvas.width - faceWidth) / 2
    const faceY = (canvas.height - faceHeight) / 2.5
    
    const basicFace = {
      x: faceX,
      y: faceY,
      width: faceWidth,
      height: faceHeight
    }
    
    // ãƒ™ãƒ¼ã‚·ãƒƒã‚¯æ¤œå‡ºã§ã‚‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æã‚’é©ç”¨
    await processFaceDetectionWithAdvancedAnalysis(basicFace, null)
    faceDetected.value = true
    
    // é«˜åº¦ãªè§£æçµæœã®æç”»
    drawAdvancedFaceAnalysis(ctx, basicFace)
  }
  
  /**
   * ğŸ¯ é©å‘½çš„é¡”æ¤œå‡ºå‡¦ç†ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒè§£æçµ±åˆç‰ˆï¼‰
   */
  const processFaceDetectionWithAdvancedAnalysis = async (face, srcMat) => {
    faceDetected.value = true
    
    // é¡”ã®åŸºæœ¬æƒ…å ±
    faceData.width = face.width
    faceData.height = face.height
    faceData.confidence = 0.8
    
    try {
      // ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒè§£æã§æ­£ç¢ºãªç›®ã®ä½ç½®ã‚’æ¤œå‡º
      const canvas = canvasElement.value
      const imageData = canvasCtx.value.getImageData(0, 0, canvas.width, canvas.height)
      
      // é¡”é ˜åŸŸã‚’ç”»åƒè§£æã‚·ã‚¹ãƒ†ãƒ ã«æ¸¡ã™
      const faceRegion = {
        x: face.x,
        y: face.y,
        width: face.width,
        height: face.height
      }
      
      // ğŸ¯ é©å‘½çš„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æå®Ÿè¡Œ
      faceAnalyzer.analyzeFace(imageData, faceRegion)
      
      // è§£æçµæœãŒæœ‰åŠ¹ãªå ´åˆã€ãã‚Œã‚’ä½¿ç”¨
      if (faceAnalyzer.faceAnalysis.eyes.isDetected) {
        console.log('ğŸ‘ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£ææˆåŠŸ - ç›®ã®ä½ç½®æ¤œå‡ºå®Œäº†')
        
        // ğŸ¯ è§£æçµæœã‚’è¦–ç·šè¿½è·¡ã«ä½¿ç”¨
        faceData.x = faceAnalyzer.faceAnalysis.eyes.center.x
        faceData.y = faceAnalyzer.faceAnalysis.eyes.center.y
        
        // ä¸¡ç›®ã®ä½ç½®ã‚‚ä¿å­˜
        faceData.leftEye = {
          x: faceAnalyzer.faceAnalysis.eyes.left.x,
          y: faceAnalyzer.faceAnalysis.eyes.left.y,
          confidence: faceAnalyzer.faceAnalysis.eyes.left.confidence
        }
        faceData.rightEye = {
          x: faceAnalyzer.faceAnalysis.eyes.right.x,
          y: faceAnalyzer.faceAnalysis.eyes.right.y,
          confidence: faceAnalyzer.faceAnalysis.eyes.right.confidence
        }
        
        // ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æãƒ™ãƒ¼ã‚¹ã®é ­éƒ¨å§¿å‹¢æ¨å®š
        calculateAdvancedHeadPose(face, faceAnalyzer.faceAnalysis.eyes)
        
      } else {
        console.log('âš ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æå¤±æ•— - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†')
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®è§£å‰–å­¦çš„æ¯”ç‡ã‚’ä½¿ç”¨
        processFaceDetectionFallback(face)
      }
      
    } catch (err) {
      console.error('âŒ é«˜åº¦è§£æã‚¨ãƒ©ãƒ¼:', err)
      // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å¾“æ¥æ‰‹æ³•ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      processFaceDetectionFallback(face)
    }
    
    // ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
    applySmoothing()
  }
  
  /**
   * å¾“æ¥ã®é¡”æ¤œå‡ºå‡¦ç†ï¼ˆç›®ã®ä½ç½®ç‰¹å®šç‰ˆï¼‰- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
   */
  const processFaceDetectionFallback = (face) => {
    // ğŸ¯ é‡è¦: ç›®ã®ä½ç½®ã‚’æ­£ç¢ºã«æ¨å®šï¼ˆé¼»ã§ã¯ãªã„ï¼‰
    const eyePositions = estimateEyePositions(face)
    
    // ç›®ã®ä¸­å¿ƒç‚¹ã‚’è¦–ç·šè¿½è·¡ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ä½¿ç”¨
    faceData.x = eyePositions.center.x
    faceData.y = eyePositions.center.y
    
    // ä¸¡ç›®ã®ä½ç½®ã‚‚ä¿å­˜ï¼ˆå°†æ¥ã®é«˜ç²¾åº¦åŒ–ç”¨ï¼‰
    faceData.leftEye = eyePositions.leftEye
    faceData.rightEye = eyePositions.rightEye
    
    // ç›®ã®ä½ç½®ã«åŸºã¥ãé ­éƒ¨å§¿å‹¢æ¨å®š
    calculateEyeBasedHeadPose(face, eyePositions)
    
    // ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    if (settings.debugMode && Date.now() % 1000 < 50) {
      console.log(`ğŸ‘ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç›®æ¤œå‡º: å·¦ç›®(${Math.round(eyePositions.leftEye.x)}, ${Math.round(eyePositions.leftEye.y)}) å³ç›®(${Math.round(eyePositions.rightEye.x)}, ${Math.round(eyePositions.rightEye.y)}) | å§¿å‹¢: Yaw=${Math.round(faceData.headPose.yaw)}Â°, Pitch=${Math.round(faceData.headPose.pitch)}Â°`)
    }
  }
  
  /**
   * ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æãƒ™ãƒ¼ã‚¹ã®é«˜åº¦ãªé ­éƒ¨å§¿å‹¢æ¨å®š
   */
  const calculateAdvancedHeadPose = (face, eyeAnalysis) => {
    const canvas = canvasElement.value
    if (!canvas) return
    
    // ç”»é¢ä¸­å¿ƒã‚’åŸºæº–ç‚¹ã¨ã™ã‚‹
    const screenCenterX = canvas.width / 2
    const screenCenterY = canvas.height / 2
    
    // ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æã«ã‚ˆã‚‹æ­£ç¢ºãªç›®ã®ä¸­å¿ƒç‚¹ã‚’ä½¿ç”¨
    const eyeCenterX = eyeAnalysis.center.x
    const eyeCenterY = eyeAnalysis.center.y
    
    // ğŸ¯ ã‚«ãƒ¡ãƒ©ã‹ã‚‰ã®è·é›¢ã‚’è€ƒæ…®ã—ãŸæ„Ÿåº¦èª¿æ•´
    const faceAreaRatio = (face.width * face.height) / (canvas.width * canvas.height)
    const distanceFactor = Math.max(0.5, Math.min(2.0, 1.0 / Math.sqrt(faceAreaRatio)))
    
    console.log(`ğŸ“ é«˜åº¦è·é›¢è£œæ­£: é¡”é¢ç©æ¯”=${faceAreaRatio.toFixed(3)}, è·é›¢ä¿‚æ•°=${distanceFactor.toFixed(2)}, ç›®è§£æä¿¡é ¼åº¦=${eyeAnalysis.left.confidence.toFixed(2)}/${eyeAnalysis.right.confidence.toFixed(2)}`)
    
    // ç›®ã®ä½ç½®ã‹ã‚‰æ­£è¦åŒ–åº§æ¨™ã‚’è¨ˆç®—
    const normalizedX = ((eyeCenterX - screenCenterX) / screenCenterX) * distanceFactor
    const normalizedY = ((eyeCenterY - screenCenterY) / screenCenterY) * distanceFactor
    
    // ç¯„å›²åˆ¶é™
    const clampedX = Math.max(-1, Math.min(1, normalizedX))
    const clampedY = Math.max(-1, Math.min(1, normalizedY))
    
    // ğŸ¯ é è·é›¢å¯¾å¿œ: ã‚ˆã‚Šå¤§ããªè§’åº¦ç¯„å›²
    faceData.headPose.yaw = clampedX * 50   // -50Â° to +50Â°ï¼ˆæ‹¡å¤§ï¼‰
    faceData.headPose.pitch = clampedY * 35 // -35Â° to +35Â°ï¼ˆæ‹¡å¤§ï¼‰
    
    // ğŸ¯ ä¸¡ç›®ã®å‚¾ãã‹ã‚‰ç´°ã‹ãªrollè§’åº¦ã‚‚æ¨å®šï¼ˆè§£æçµæœã‚’æ´»ç”¨ï¼‰
    if (eyeAnalysis.left.confidence > 0.3 && eyeAnalysis.right.confidence > 0.3) {
      const eyeAngle = Math.atan2(
        eyeAnalysis.right.y - eyeAnalysis.left.y,
        eyeAnalysis.right.x - eyeAnalysis.left.x
      )
      faceData.headPose.roll = (eyeAngle * 180 / Math.PI) * 0.5 // è»½å¾®ãªèª¿æ•´
    } else {
      faceData.headPose.roll = 0
    }
  }
  
  /**
   * å¾“æ¥ã®é¡”æ¤œå‡ºå‡¦ç†ï¼ˆç›®ã®ä½ç½®ç‰¹å®šç‰ˆï¼‰- æ—§ç‰ˆä¿æŒ
   */
  const processFaceDetection = (face) => {
    faceData.height = face.height
    faceData.confidence = 0.8
    
    // ğŸ¯ é‡è¦: ç›®ã®ä½ç½®ã‚’æ­£ç¢ºã«æ¨å®šï¼ˆé¼»ã§ã¯ãªã„ï¼‰
    const eyePositions = estimateEyePositions(face)
    
    // ç›®ã®ä¸­å¿ƒç‚¹ã‚’è¦–ç·šè¿½è·¡ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦ä½¿ç”¨
    faceData.x = eyePositions.center.x
    faceData.y = eyePositions.center.y
    
    // ä¸¡ç›®ã®ä½ç½®ã‚‚ä¿å­˜ï¼ˆå°†æ¥ã®é«˜ç²¾åº¦åŒ–ç”¨ï¼‰
    faceData.leftEye = eyePositions.leftEye
    faceData.rightEye = eyePositions.rightEye
    
    // ç›®ã®ä½ç½®ã«åŸºã¥ãé ­éƒ¨å§¿å‹¢æ¨å®š
    calculateEyeBasedHeadPose(face, eyePositions)
    
    // ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
    applySmoothing()
    
    // ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    if (settings.debugMode && Date.now() % 1000 < 50) {
      console.log(`ğŸ‘ï¸ ç›®æ¤œå‡º: å·¦ç›®(${Math.round(eyePositions.leftEye.x)}, ${Math.round(eyePositions.leftEye.y)}) å³ç›®(${Math.round(eyePositions.rightEye.x)}, ${Math.round(eyePositions.rightEye.y)}) | å§¿å‹¢: Yaw=${Math.round(faceData.headPose.yaw)}Â°, Pitch=${Math.round(faceData.headPose.pitch)}Â°`)
    }
  }
  
  /**
   * ğŸ¯ ç›®ã®ä½ç½®æ¨å®šï¼ˆé¡”æ¤œå‡ºçµæœã‹ã‚‰æ­£ç¢ºãªç›®ã®ä½ç½®ã‚’è¨ˆç®—ï¼‰
   */
  const estimateEyePositions = (face) => {
    // é¡”ã®è§£å‰–å­¦çš„æ¯”ç‡ã«åŸºã¥ãç›®ã®ä½ç½®æ¨å®š
    const faceWidth = face.width
    const faceHeight = face.height
    const faceX = face.x
    const faceY = face.y
    
    // ç›®ã®ä½ç½®ã®æ¨™æº–çš„ãªæ¯”ç‡ï¼ˆé¡”èªè­˜ç ”ç©¶ã«ã‚ˆã‚‹ï¼‰
    const eyeYRatio = 0.35      // é¡”ã®ä¸Šã‹ã‚‰35%ã®ä½ç½®
    const leftEyeXRatio = 0.23   // å·¦ç«¯ã‹ã‚‰23%ã®ä½ç½®
    const rightEyeXRatio = 0.77  // å·¦ç«¯ã‹ã‚‰77%ã®ä½ç½®
    
    // ç›®ã®ä½ç½®è¨ˆç®—
    const leftEye = {
      x: faceX + faceWidth * leftEyeXRatio,
      y: faceY + faceHeight * eyeYRatio
    }
    
    const rightEye = {
      x: faceX + faceWidth * rightEyeXRatio,
      y: faceY + faceHeight * eyeYRatio
    }
    
    // ä¸¡ç›®ã®ä¸­å¿ƒç‚¹ï¼ˆè¦–ç·šè¿½è·¡ã®åŸºæº–ç‚¹ï¼‰
    const center = {
      x: (leftEye.x + rightEye.x) / 2,
      y: (leftEye.y + rightEye.y) / 2
    }
    
    return {
      leftEye,
      rightEye,
      center,
      eyeDistance: Math.abs(rightEye.x - leftEye.x)
    }
  }
  
  /**
   * ğŸ¯ ç›®ãƒ™ãƒ¼ã‚¹ã®é ­éƒ¨å§¿å‹¢æ¨å®šï¼ˆé è·é›¢å¯¾å¿œãƒ»é«˜ç²¾åº¦ç‰ˆï¼‰
   */
  const calculateEyeBasedHeadPose = (face, eyePositions) => {
    const canvas = canvasElement.value
    if (!canvas) return
    
    // ç”»é¢ä¸­å¿ƒã‚’åŸºæº–ç‚¹ã¨ã™ã‚‹
    const screenCenterX = canvas.width / 2
    const screenCenterY = canvas.height / 2
    
    // ä¸¡ç›®ã®ä¸­å¿ƒç‚¹ã‚’ä½¿ç”¨ï¼ˆé¼»ã§ã¯ãªã„ï¼‰
    const eyeCenterX = eyePositions.center.x
    const eyeCenterY = eyePositions.center.y
    
    // ğŸ¯ ã‚«ãƒ¡ãƒ©ã‹ã‚‰ã®è·é›¢ã‚’è€ƒæ…®ã—ãŸæ„Ÿåº¦èª¿æ•´
    const faceAreaRatio = (face.width * face.height) / (canvas.width * canvas.height)
    const distanceFactor = Math.max(0.5, Math.min(2.0, 1.0 / Math.sqrt(faceAreaRatio)))
    
    console.log(`ğŸ“ è·é›¢è£œæ­£: é¡”é¢ç©æ¯”=${faceAreaRatio.toFixed(3)}, è·é›¢ä¿‚æ•°=${distanceFactor.toFixed(2)}`)
    
    // ç›®ã®ä½ç½®ã‹ã‚‰æ­£è¦åŒ–åº§æ¨™ã‚’è¨ˆç®—
    const normalizedX = ((eyeCenterX - screenCenterX) / screenCenterX) * distanceFactor
    const normalizedY = ((eyeCenterY - screenCenterY) / screenCenterY) * distanceFactor
    
    // ç¯„å›²åˆ¶é™
    const clampedX = Math.max(-1, Math.min(1, normalizedX))
    const clampedY = Math.max(-1, Math.min(1, normalizedY))
    
    // ğŸ¯ é è·é›¢å¯¾å¿œ: ã‚ˆã‚Šå¤§ããªè§’åº¦ç¯„å›²
    faceData.headPose.yaw = clampedX * 50   // -50Â° to +50Â°ï¼ˆæ‹¡å¤§ï¼‰
    faceData.headPose.pitch = clampedY * 35 // -35Â° to +35Â°ï¼ˆæ‹¡å¤§ï¼‰
    faceData.headPose.roll = 0
    
    // ğŸ¯ ä¸¡ç›®ã®å‚¾ãã‹ã‚‰ç´°ã‹ãªrollè§’åº¦ã‚‚æ¨å®š
    const eyeAngle = Math.atan2(
      eyePositions.rightEye.y - eyePositions.leftEye.y,
      eyePositions.rightEye.x - eyePositions.leftEye.x
    )
    faceData.headPose.roll = (eyeAngle * 180 / Math.PI) * 0.5 // è»½å¾®ãªèª¿æ•´
  }
  
  /**
   * æ”¹è‰¯ç‰ˆé ­éƒ¨å§¿å‹¢æ¨å®šï¼ˆæ—§ç‰ˆãƒ»å‚è€ƒç”¨ï¼‰
   */
  const calculateAccurateHeadPose = (face) => {
    const canvas = canvasElement.value
    if (!canvas) return
    
    const faceCenterX = face.x + face.width / 2
    const faceCenterY = face.y + face.height / 2
    
    const screenCenterX = canvas.width / 2
    const screenCenterY = canvas.height / 2
    
    // ã‚ˆã‚Šæ­£ç¢ºãªæ­£è¦åŒ–ï¼ˆé¡”ã®ã‚µã‚¤ã‚ºã‚‚è€ƒæ…®ï¼‰
    const faceWidthRatio = face.width / canvas.width
    const faceHeightRatio = face.height / canvas.height
    
    // é¡”ã®ã‚µã‚¤ã‚ºã«åŸºã¥ãæ„Ÿåº¦èª¿æ•´
    const sensitivityX = Math.max(0.5, Math.min(2.0, 1.0 / faceWidthRatio))
    const sensitivityY = Math.max(0.5, Math.min(2.0, 1.0 / faceHeightRatio))
    
    // ä½ç½®ã®æ­£è¦åŒ– (-1 to 1)
    const normalizedX = ((faceCenterX - screenCenterX) / screenCenterX) * sensitivityX
    const normalizedY = ((faceCenterY - screenCenterY) / screenCenterY) * sensitivityY
    
    // ç¯„å›²åˆ¶é™
    const clampedX = Math.max(-1, Math.min(1, normalizedX))
    const clampedY = Math.max(-1, Math.min(1, normalizedY))
    
    // é ­éƒ¨å§¿å‹¢ã«å¤‰æ›ï¼ˆåº¦å˜ä½ï¼‰
    faceData.headPose.yaw = clampedX * 35   // -35Â° to +35Â°
    faceData.headPose.pitch = clampedY * 25 // -25Â° to +25Â°
    faceData.headPose.roll = 0
  }
  
  /**
   * ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
   */
  let previousPose = { yaw: 0, pitch: 0, roll: 0 }
  
  const applySmoothing = () => {
    const factor = settings.smoothingFactor
    
    faceData.headPose.yaw = lerp(previousPose.yaw, faceData.headPose.yaw, factor)
    faceData.headPose.pitch = lerp(previousPose.pitch, faceData.headPose.pitch, factor)
    faceData.headPose.roll = lerp(previousPose.roll, faceData.headPose.roll, factor)
    
    previousPose = { ...faceData.headPose }
  }
  
  /**
   * ç·šå½¢è£œé–“
   */
  const lerp = (a, b, t) => a + (b - a) * t
  
  /**
   * ğŸ¯ é«˜åº¦ãªè§£æçµæœæç”»ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒè§£æçµ±åˆç‰ˆï¼‰
   */
  const drawAdvancedFaceAnalysis = (ctx, face) => {
    // é¡”ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
    ctx.strokeStyle = '#00ff00'
    ctx.lineWidth = 2
    ctx.strokeRect(face.x, face.y, face.width, face.height)
    
    // ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æçµæœã‚’æç”»
    if (faceAnalyzer.faceAnalysis.eyes.isDetected) {
      // é«˜åº¦è§£æã‚·ã‚¹ãƒ†ãƒ ã®æç”»ã‚’ä½¿ç”¨
      faceAnalyzer.drawAdvancedAnalysis(ctx, canvasElement.value)
      
      // è¿½åŠ ã®çµ±åˆæƒ…å ±
      ctx.fillStyle = '#ff0080'
      ctx.font = '12px Arial'
      ctx.fillText('ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æãƒ¢ãƒ¼ãƒ‰', 10, 260)
      ctx.fillText(`è§£ææ™‚é–“: ${faceAnalyzer.faceAnalysis.analysisTime.toFixed(1)}ms`, 10, 280)
      
    } else {
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ç›®ã®ä½ç½®æ¨å®šã‚’è¡¨ç¤º
      drawFallbackEyePositions(ctx, face)
      
      ctx.fillStyle = '#ffaa00'
      ctx.font = '12px Arial'
      ctx.fillText('âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰', 10, 260)
    }
    
    // ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
    ctx.fillStyle = '#00ff00'
    ctx.font = '14px Arial'
    ctx.fillText(`${stats.detectionMethod} + ç”»åƒè§£æ | ä¿¡é ¼åº¦: ${Math.round(faceData.confidence * 100)}%`, 10, 25)
    ctx.fillText(`FPS: ${stats.fps}`, 10, 45)
    ctx.fillText(`ç›®ã®å§¿å‹¢ - Yaw: ${Math.round(faceData.headPose.yaw)}Â°, Pitch: ${Math.round(faceData.headPose.pitch)}Â°`, 10, 65)
    ctx.fillText(`è·é›¢è£œæ­£: ${((face.width * face.height) / (ctx.canvas.width * ctx.canvas.height)).toFixed(3)}`, 10, 85)
  }
  
  /**
   * ğŸ¯ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç›®ã®ä½ç½®æç”»
   */
  const drawFallbackEyePositions = (ctx, face) => {
    // ç›®ã®ä½ç½®ã‚’æ¨å®šã—ã¦æç”»
    const eyePositions = estimateEyePositions(face)
    
    // å·¦ç›®æç”»
    ctx.fillStyle = '#00ffff'  // ã‚·ã‚¢ãƒ³
    ctx.beginPath()
    ctx.arc(eyePositions.leftEye.x, eyePositions.leftEye.y, 6, 0, 2 * Math.PI)
    ctx.fill()
    
    // å³ç›®æç”»
    ctx.fillStyle = '#00ffff'  // ã‚·ã‚¢ãƒ³
    ctx.beginPath()
    ctx.arc(eyePositions.rightEye.x, eyePositions.rightEye.y, 6, 0, 2 * Math.PI)
    ctx.fill()
    
    // ä¸¡ç›®ã®ä¸­å¿ƒç‚¹ï¼ˆè¦–ç·šè¿½è·¡åŸºæº–ç‚¹ï¼‰
    ctx.fillStyle = '#ff0080'  // ãƒ”ãƒ³ã‚¯ï¼ˆç›®ç«‹ã¤è‰²ï¼‰
    ctx.beginPath()
    ctx.arc(eyePositions.center.x, eyePositions.center.y, 8, 0, 2 * Math.PI)
    ctx.fill()
    
    // ç›®ã®ä¸­å¿ƒã¨é¡”ã®ä¸­å¿ƒã®é•ã„ã‚’ç·šã§è¡¨ç¤º
    const faceCenterX = face.x + face.width / 2
    const faceCenterY = face.y + face.height / 2
    
    ctx.strokeStyle = '#ffff00'  // é»„è‰²
    ctx.lineWidth = 2
    ctx.beginPath()
    ctx.moveTo(faceCenterX, faceCenterY)
    ctx.lineTo(eyePositions.center.x, eyePositions.center.y)
    ctx.stroke()
    
    // èª¬æ˜
    ctx.fillStyle = '#ff0080'
    ctx.font = '12px Arial'
    ctx.fillText('ãƒ”ãƒ³ã‚¯=ç›®ã®ä¸­å¿ƒï¼ˆè¦–ç·šåŸºæº–ï¼‰', 10, 105)
    ctx.fillStyle = '#00ffff'
    ctx.fillText('ã‚·ã‚¢ãƒ³=å·¦å³ã®ç›®', 10, 120)
    ctx.fillStyle = '#ffff00'
    ctx.fillText('é»„ç·š=é¡”ä¸­å¿ƒâ†’ç›®ä¸­å¿ƒ', 10, 135)
  }
  
  /**
   * ğŸ¯ æ”¹è‰¯ç‰ˆãƒ‡ãƒãƒƒã‚°æç”»ï¼ˆç›®ã®ä½ç½®è¡¨ç¤ºï¼‰- æ—§ç‰ˆä¿æŒ
   */
  const drawFaceRect = (ctx, face) => {
    // é¡”ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹
    ctx.strokeStyle = '#00ff00'
    ctx.lineWidth = 2
    ctx.strokeRect(face.x, face.y, face.width, face.height)
    
    // ğŸ¯ ç›®ã®ä½ç½®ã‚’æ¨å®šã—ã¦æç”»
    const eyePositions = estimateEyePositions(face)
    
    // å·¦ç›®æç”»
    ctx.fillStyle = '#00ffff'  // ã‚·ã‚¢ãƒ³
    ctx.beginPath()
    ctx.arc(eyePositions.leftEye.x, eyePositions.leftEye.y, 6, 0, 2 * Math.PI)
    ctx.fill()
    
    // å³ç›®æç”»
    ctx.fillStyle = '#00ffff'  // ã‚·ã‚¢ãƒ³
    ctx.beginPath()
    ctx.arc(eyePositions.rightEye.x, eyePositions.rightEye.y, 6, 0, 2 * Math.PI)
    ctx.fill()
    
    // ä¸¡ç›®ã®ä¸­å¿ƒç‚¹ï¼ˆè¦–ç·šè¿½è·¡åŸºæº–ç‚¹ï¼‰
    ctx.fillStyle = '#ff0080'  // ãƒ”ãƒ³ã‚¯ï¼ˆç›®ç«‹ã¤è‰²ï¼‰
    ctx.beginPath()
    ctx.arc(eyePositions.center.x, eyePositions.center.y, 8, 0, 2 * Math.PI)
    ctx.fill()
    
    // ç›®ã®ä¸­å¿ƒã¨é¡”ã®ä¸­å¿ƒã®é•ã„ã‚’ç·šã§è¡¨ç¤º
    const faceCenterX = face.x + face.width / 2
    const faceCenterY = face.y + face.height / 2
    
    ctx.strokeStyle = '#ffff00'  // é»„è‰²
    ctx.lineWidth = 2
    ctx.beginPath()
    ctx.moveTo(faceCenterX, faceCenterY)
    ctx.lineTo(eyePositions.center.x, eyePositions.center.y)
    ctx.stroke()
    
    // æƒ…å ±è¡¨ç¤º
    ctx.fillStyle = '#00ff00'
    ctx.font = '14px Arial'
    ctx.fillText(`${stats.detectionMethod} | ä¿¡é ¼åº¦: ${Math.round(faceData.confidence * 100)}%`, 10, 25)
    ctx.fillText(`FPS: ${stats.fps}`, 10, 45)
    ctx.fillText(`ç›®ã®å§¿å‹¢ - Yaw: ${Math.round(faceData.headPose.yaw)}Â°, Pitch: ${Math.round(faceData.headPose.pitch)}Â°`, 10, 65)
    ctx.fillText(`è·é›¢è£œæ­£: ${((face.width * face.height) / (ctx.canvas.width * ctx.canvas.height)).toFixed(3)}`, 10, 85)
    
    // ğŸ¯ è¦–ç·šè¿½è·¡ãƒã‚¤ãƒ³ãƒˆã®èª¬æ˜
    ctx.fillStyle = '#ff0080'
    ctx.font = '12px Arial'
    ctx.fillText('ãƒ”ãƒ³ã‚¯=ç›®ã®ä¸­å¿ƒï¼ˆè¦–ç·šåŸºæº–ï¼‰', 10, 105)
    ctx.fillStyle = '#00ffff'
    ctx.fillText('ã‚·ã‚¢ãƒ³=å·¦å³ã®ç›®', 10, 120)
    ctx.fillStyle = '#ffff00'
    ctx.fillText('é»„ç·š=é¡”ä¸­å¿ƒâ†’ç›®ä¸­å¿ƒ', 10, 135)
  }
  
  /**
   * ãƒ•ã‚§ã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ
   */
  const resetFaceData = () => {
    faceData.x = 0
    faceData.y = 0
    faceData.width = 0
    faceData.height = 0
    faceData.confidence = 0
    faceData.headPose = { yaw: 0, pitch: 0, roll: 0 }
  }
  
  /**
   * çµ±è¨ˆæ›´æ–°
   */
  const updateStats = () => {
    stats.frameCount++
    const now = Date.now()
    
    if (now - stats.lastUpdate >= 1000) {
      stats.fps = Math.round(stats.frameCount * 1000 / (now - stats.lastUpdate))
      stats.frameCount = 0
      stats.lastUpdate = now
    }
  }
  
  /**
   * è¿½è·¡åœæ­¢
   */
  const stopTracking = () => {
    isTracking.value = false
    
    if (animationFrame) {
      cancelAnimationFrame(animationFrame)
      animationFrame = null
    }
    
    if (videoElement.value && videoElement.value.srcObject) {
      const tracks = videoElement.value.srcObject.getTracks()
      tracks.forEach(track => track.stop())
      videoElement.value.srcObject = null
    }
    
    resetFaceData()
    console.log('â¹ï¸ OpenCVè¦–ç·šè¿½è·¡åœæ­¢')
  }
  
  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  onUnmounted(() => {
    stopTracking()
    
    // OpenCVãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (classifier) {
      classifier.delete()
    }
  })
  
  return {
    // çŠ¶æ…‹
    isInitialized,
    isTracking,
    faceDetected,
    error,
    faceData,
    settings,
    stats,
    
    // ğŸ¯ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒè§£æã‚·ã‚¹ãƒ†ãƒ 
    faceAnalyzer,
    
    // ãƒ¡ã‚½ãƒƒãƒ‰
    initializeOpenCV,
    startTracking,
    stopTracking,
    startCamera
  }
}