import { ref, reactive, onMounted, onUnmounted } from 'vue'

/**
 * MediaPipeé¡”æ¤œå‡ºãƒ™ãƒ¼ã‚¹ã®è¦–ç·šè¿½è·¡ã‚·ã‚¹ãƒ†ãƒ 
 * WebGazer.jsã®å®Œå…¨ä»£æ›¿ - é«˜ç²¾åº¦ãƒ»é«˜å®‰å®šæ€§ãƒ»AACæœ€é©åŒ–
 */
export function useMediaPipeFaceTracker() {
  // çŠ¶æ…‹ç®¡ç†
  const isInitialized = ref(false)
  const isTracking = ref(false)
  const error = ref(null)
  const faceDetected = ref(false)
  
  // é¡”æ¤œå‡ºãƒ‡ãƒ¼ã‚¿
  const faceData = reactive({
    x: 0,           // é¡”ä¸­å¿ƒXåº§æ¨™
    y: 0,           // é¡”ä¸­å¿ƒYåº§æ¨™
    width: 0,       // é¡”ã®å¹…
    height: 0,      // é¡”ã®é«˜ã•
    confidence: 0,  // æ¤œå‡ºä¿¡é ¼åº¦
    headPose: {     // é ­éƒ¨å§¿å‹¢
      yaw: 0,       // å·¦å³å›žè»¢
      pitch: 0,     // ä¸Šä¸‹å›žè»¢
      roll: 0       // å‚¾ã
    }
  })
  
  // ã‚«ãƒ¡ãƒ©ã¨Canvasè¦ç´ 
  const videoElement = ref(null)
  const canvasElement = ref(null)
  const canvasCtx = ref(null)
  
  // MediaPipeè¨­å®š
  const settings = reactive({
    modelSelection: 0,      // 0: 2mä»¥å†…ç”¨, 1: 5mä»¥å†…ç”¨
    minDetectionConfidence: 0.7,  // æ¤œå‡ºä¿¡é ¼åº¦é–¾å€¤
    minTrackingConfidence: 0.5,   // è¿½è·¡ä¿¡é ¼åº¦é–¾å€¤
    maxNumFaces: 1,               // æœ€å¤§æ¤œå‡ºé¡”æ•°
    smoothing: true,              // ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æœ‰åŠ¹
    smoothingFactor: 0.3          // ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°
  })
  
  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ç›£è¦–
  const stats = reactive({
    fps: 0,
    detectionCount: 0,
    avgConfidence: 0,
    lastUpdate: Date.now()
  })
  
  // MediaPipe Face Detection ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
  let faceDetection = null
  let animationFrame = null
  
  /**
   * MediaPipe Face Detectionã‚’åˆæœŸåŒ–
   */
  const initializeFaceDetection = async () => {
    try {
      console.log('ðŸš€ MediaPipe Face DetectionåˆæœŸåŒ–é–‹å§‹')
      
      // å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆCDNå¯¾å¿œï¼‰
      const { FaceDetection } = await import('@mediapipe/face_detection')
      const { Camera } = await import('@mediapipe/camera_utils')
      
      // Face DetectionåˆæœŸåŒ–
      faceDetection = new FaceDetection({
        locateFile: (file) => {
          return `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection/${file}`
        }
      })
      
      // ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
      faceDetection.setOptions({
        model: settings.modelSelection === 0 ? 'short' : 'full',
        minDetectionConfidence: settings.minDetectionConfidence,
        minTrackingConfidence: settings.minTrackingConfidence
      })
      
      // æ¤œå‡ºçµæžœã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
      faceDetection.onResults(onFaceDetectionResults)
      
      console.log('âœ… MediaPipe Face DetectionåˆæœŸåŒ–å®Œäº†')
      isInitialized.value = true
      
    } catch (err) {
      console.error('âŒ MediaPipeåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', err)
      error.value = `MediaPipeåˆæœŸåŒ–å¤±æ•—: ${err.message}`
      
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: WebRTCåŸºæœ¬æ¤œå‡º
      await initializeFallbackDetection()
    }
  }
  
  /**
   * MediaPipeæ¤œå‡ºçµæžœå‡¦ç†
   */
  const onFaceDetectionResults = (results) => {
    updateStats()
    
    if (results.detections && results.detections.length > 0) {
      const detection = results.detections[0] // æœ€åˆã®é¡”ã‚’ä½¿ç”¨
      
      faceDetected.value = true
      
      // é¡”ã®ä½ç½®ãƒ»ã‚µã‚¤ã‚ºæ›´æ–°
      const bbox = detection.boundingBox
      faceData.x = (bbox.xCenter * videoElement.value.videoWidth) || 0
      faceData.y = (bbox.yCenter * videoElement.value.videoHeight) || 0
      faceData.width = (bbox.width * videoElement.value.videoWidth) || 0
      faceData.height = (bbox.height * videoElement.value.videoHeight) || 0
      faceData.confidence = detection.score || 0
      
      // é ­éƒ¨å§¿å‹¢æŽ¨å®šï¼ˆå˜ç´”åŒ–ï¼‰
      calculateHeadPose(detection)
      
      // ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
      if (settings.smoothing) {
        applySmoothingToFaceData()
      }
      
      // ãƒ‡ãƒãƒƒã‚°æç”»
      if (canvasElement.value && canvasCtx.value) {
        drawFaceDetection(detection)
      }
      
    } else {
      faceDetected.value = false
      resetFaceData()
    }
  }
  
  /**
   * é ­éƒ¨å§¿å‹¢ã®ç°¡æ˜“æŽ¨å®š
   */
  const calculateHeadPose = (detection) => {
    if (!detection.landmarks || detection.landmarks.length < 6) {
      return
    }
    
    // ãƒ©ãƒ³ãƒ‰ãƒžãƒ¼ã‚¯ã‹ã‚‰å§¿å‹¢æŽ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
    const landmarks = detection.landmarks
    const nose = landmarks[2] // é¼»å…ˆ
    const leftEye = landmarks[0] // å·¦ç›®
    const rightEye = landmarks[1] // å³ç›®
    
    // Yawï¼ˆå·¦å³å›žè»¢ï¼‰: ç›®ã®ä½ç½®ã‹ã‚‰æŽ¨å®š
    const eyeCenterX = (leftEye.x + rightEye.x) / 2
    const yawRadians = Math.atan2(nose.x - eyeCenterX, 0.1)
    faceData.headPose.yaw = yawRadians * (180 / Math.PI)
    
    // Pitchï¼ˆä¸Šä¸‹å›žè»¢ï¼‰: é¼»ã¨ç›®ã®åž‚ç›´é–¢ä¿‚ã‹ã‚‰æŽ¨å®š  
    const eyeCenterY = (leftEye.y + rightEye.y) / 2
    const pitchRadians = Math.atan2(nose.y - eyeCenterY, 0.1)
    faceData.headPose.pitch = pitchRadians * (180 / Math.PI)
    
    // Rollï¼ˆå‚¾ãï¼‰: ç›®ã®æ°´å¹³é–¢ä¿‚ã‹ã‚‰æŽ¨å®š
    const rollRadians = Math.atan2(rightEye.y - leftEye.y, rightEye.x - leftEye.x)
    faceData.headPose.roll = rollRadians * (180 / Math.PI)
  }
  
  /**
   * ãƒ•ã‚§ã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
   */
  const applySmoothingToFaceData = () => {
    const factor = settings.smoothingFactor
    // å˜ç´”åŒ–: ç¾åœ¨ã¯ä½ç½®ã®ã¿ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    // å®Ÿè£…æ™‚ã«ã¯å‰å›žå€¤ã¨ã®åŠ é‡å¹³å‡ã‚’è¨ˆç®—
  }
  
  /**
   * ãƒ•ã‚§ã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
   */
  const resetFaceData = () => {
    faceData.x = 0
    faceData.y = 0
    faceData.width = 0
    faceData.height = 0
    faceData.confidence = 0
    faceData.headPose = { yaw: 0, pitch: 0, roll: 0 }
  }
  
  /**
   * ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹çµ±è¨ˆæ›´æ–°
   */
  const updateStats = () => {
    const now = Date.now()
    const elapsed = now - stats.lastUpdate
    
    if (elapsed >= 1000) { // 1ç§’ã”ã¨ã«æ›´æ–°
      stats.fps = Math.round(stats.detectionCount * 1000 / elapsed)
      stats.detectionCount = 0
      stats.lastUpdate = now
    } else {
      stats.detectionCount++
    }
  }
  
  /**
   * Canvasä¸Šã«æ¤œå‡ºçµæžœã‚’æç”»ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
   */
  const drawFaceDetection = (detection) => {
    const ctx = canvasCtx.value
    const canvas = canvasElement.value
    
    // ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚¯ãƒªã‚¢
    ctx.clearRect(0, 0, canvas.width, canvas.height)
    
    // ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æç”»
    const bbox = detection.boundingBox
    ctx.strokeStyle = '#00ff00'
    ctx.lineWidth = 2
    ctx.strokeRect(
      bbox.xCenter * canvas.width - (bbox.width * canvas.width) / 2,
      bbox.yCenter * canvas.height - (bbox.height * canvas.height) / 2,
      bbox.width * canvas.width,
      bbox.height * canvas.height
    )
    
    // ä¿¡é ¼åº¦è¡¨ç¤º
    ctx.fillStyle = '#00ff00'
    ctx.font = '16px Arial'
    ctx.fillText(
      `ä¿¡é ¼åº¦: ${Math.round(detection.score * 100)}%`,
      10, 30
    )
    
    // FPSè¡¨ç¤º
    ctx.fillText(`FPS: ${stats.fps}`, 10, 50)
  }
  
  /**
   * ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹
   */
  const startCamera = async (constraints = {}) => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          frameRate: { ideal: 30 },
          ...constraints
        }
      })
      
      if (videoElement.value) {
        videoElement.value.srcObject = stream
        await videoElement.value.play()
        
        console.log('ðŸ“¹ ã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹:', {
          width: videoElement.value.videoWidth,
          height: videoElement.value.videoHeight
        })
      }
      
      return stream
    } catch (err) {
      console.error('âŒ ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼:', err)
      error.value = `ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: ${err.message}`
      throw err
    }
  }
  
  /**
   * è¦–ç·šè¿½è·¡é–‹å§‹
   */
  const startTracking = async (videoEl, canvasEl) => {
    try {
      if (!isInitialized.value) {
        await initializeFaceDetection()
      }
      
      videoElement.value = videoEl
      canvasElement.value = canvasEl
      
      if (canvasEl) {
        canvasCtx.value = canvasEl.getContext('2d')
      }
      
      await startCamera()
      
      // MediaPipeã‚«ãƒ¡ãƒ©é–‹å§‹
      if (faceDetection) {
        const camera = new Camera(videoElement.value, {
          onFrame: async () => {
            if (faceDetection && isTracking.value) {
              await faceDetection.send({ image: videoElement.value })
            }
          },
          width: 1280,
          height: 720
        })
        
        camera.start()
      }
      
      isTracking.value = true
      console.log('âœ… MediaPipeè¦–ç·šè¿½è·¡é–‹å§‹')
      
    } catch (err) {
      console.error('âŒ è¿½è·¡é–‹å§‹ã‚¨ãƒ©ãƒ¼:', err)
      error.value = `è¿½è·¡é–‹å§‹å¤±æ•—: ${err.message}`
    }
  }
  
  /**
   * è¦–ç·šè¿½è·¡åœæ­¢
   */
  const stopTracking = () => {
    isTracking.value = false
    
    if (animationFrame) {
      cancelAnimationFrame(animationFrame)
      animationFrame = null
    }
    
    if (videoElement.value && videoElement.value.srcObject) {
      const tracks = videoElement.value.srcObject.getTracks()
      tracks.forEach(track => track.stop())
      videoElement.value.srcObject = null
    }
    
    resetFaceData()
    console.log('â¹ï¸ MediaPipeè¦–ç·šè¿½è·¡åœæ­¢')
  }
  
  /**
   * ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œå‡ºï¼ˆMediaPipeå¤±æ•—æ™‚ï¼‰
   */
  const initializeFallbackDetection = async () => {
    console.log('ðŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ')
    // åŸºæœ¬çš„ãªWebRTCé¡”æ¤œå‡ºã®å®Ÿè£…
    // å°†æ¥çš„ã«ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªCSSãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡ºãªã©
    error.value = 'MediaPipeåˆ©ç”¨ä¸å¯ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ä½¿ç”¨ä¸­'
  }
  
  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  onUnmounted(() => {
    stopTracking()
    if (faceDetection) {
      faceDetection.close()
    }
  })
  
  return {
    // çŠ¶æ…‹
    isInitialized,
    isTracking,
    error,
    faceDetected,
    faceData,
    settings,
    stats,
    
    // ãƒ¡ã‚½ãƒƒãƒ‰
    initializeFaceDetection,
    startTracking,
    stopTracking,
    startCamera
  }
}